import numpy as np
import time
import random
import matplotlib.pyplot as plt
import json

# ==============================================================================
# Ø§Ù„Ù‚Ø³Ù… 1: Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (Node, nn) ÙˆØ§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
# ==============================================================================
# ... (Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆØ§Ù„Ù…Ø³ØªÙ‚Ø± Ù„ÙØ¦Ø© Node ÙˆØ¬Ù…ÙŠØ¹ ÙØ¦Ø§Øª nn Ù…Ø«Ù„ Linear, PReLU, Tanh, Sequential)
# ... (Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ø¯ÙˆØ§Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ù…Ø«Ù„ constant_lr, linear_decay_lr, etc.)
# ... (Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ø¯Ø§Ù„Ø© create_dynamic_model)
def _sum_to_shape(grad, shape):
    while grad.ndim > len(shape): grad = grad.sum(axis=0)
    for i, (g_dim, t_dim) in enumerate(zip(grad.shape, shape)):
        if t_dim == 1 and g_dim != 1: grad = grad.sum(axis=i, keepdims=True)
    return grad.reshape(shape)
class Node:
    def __init__(self, value, parents=(), op=''):
        self.value = np.array(value, dtype=float); self.parents = parents; self.op = op
        self.grad = None; self._backward = lambda: None
    def _ensure(self, other):
        return other if isinstance(other, Node) else Node(np.array(other, dtype=float))
    def __add__(self, other):
        other = self._ensure(other); out = Node(self.value + other.value, (self, other), '+')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad, self.value.shape)
            other.grad += _sum_to_shape(out.grad, other.value.shape)
        out._backward = _backward; return out
    def __mul__(self, other):
        other = self._ensure(other); out = Node(self.value * other.value, (self, other), '*')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad * other.value, self.value.shape)
            other.grad += _sum_to_shape(out.grad * self.value, other.value.shape)
        out._backward = _backward; return out
    def __matmul__(self, other):
        other = self._ensure(other); out = Node(self.value @ other.value, (self, other), '@')
        def _backward():
            if out.grad is None: return
            A, B, G = self.value, other.value, out.grad
            self.grad += _sum_to_shape(G @ B.T, self.value.shape)
            other.grad += _sum_to_shape(A.T @ G, other.value.shape)
        out._backward = _backward; return out
    def sum(self):
        out = Node(self.value.sum(), (self,), 'sum')
        def _backward():
            if out.grad is None: return
            self.grad += np.broadcast_to(out.grad, self.value.shape)
        out._backward = _backward; return out
    def backward(self):
        topo, visited = [], set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for p in v.parents: build_topo(p)
                topo.append(v)
        build_topo(self)
        for v in topo: v.grad = np.zeros_like(v.value)
        self.grad = np.ones_like(self.value)
        for v in reversed(topo): v._backward()

class nn:
    class Module:
        def parameters(self): yield from []
        def __call__(self, *args, **kwargs): return self.forward(*args, **kwargs)
        def zero_grad(self):
            for p in self.parameters(): p.grad = np.zeros_like(p.value)
    class Linear(Module):
        def __init__(self, in_features, out_features):
            self.in_features = in_features
            limit = np.sqrt(1 / self.in_features)
            self.weight = Node(np.random.randn(in_features, out_features) * limit)
            self.bias = Node(np.zeros(out_features))
        def forward(self, x): return x @ self.weight + self.bias
        def parameters(self): yield from [self.weight, self.bias]
    class PReLU(Module):
        def __init__(self, initial_alpha=0.01):
            self.alpha = Node(np.array([initial_alpha]))
        def forward(self, x):
            positive_part = Node(np.maximum(0, x.value)); negative_part = Node(np.minimum(0, x.value))
            out = positive_part + (negative_part * self.alpha); out.parents = (x, self.alpha); out.op = 'prelu'
            def _backward():
                if out.grad is None: return
                grad_x = (x.value > 0) * 1.0 + (x.value <= 0) * self.alpha.value
                x.grad += grad_x * out.grad
                grad_alpha = (x.value * (x.value <= 0)) * out.grad
                self.alpha.grad += grad_alpha.sum()
            out._backward = _backward; return out
        def parameters(self): yield self.alpha
    class Tanh(Module):
        def forward(self, x):
            out = Node(np.tanh(x.value), parents=(x,), op='tanh')
            def _backward():
                if out.grad is None: return
                x.grad += (1 - out.value**2) * out.grad
            out._backward = _backward; return out
    class Sequential(Module):
        def __init__(self, *layers): self.layers = layers
        def forward(self, x):
            for layer in self.layers: x = layer(x)
            return x
        def parameters(self):
            for layer in self.layers: yield from layer.parameters()

def constant_lr(epoch, initial_lr, **kwargs): return initial_lr
def linear_decay_lr(epoch, initial_lr, total_epochs, **kwargs): return initial_lr * (1 - (epoch / total_epochs))
def exponential_decay_lr(epoch, initial_lr, decay_rate, **kwargs): return initial_lr * (decay_rate ** epoch)
def cyclical_lr(epoch, initial_lr, max_lr, step_size, **kwargs):
    cycle = np.floor(1 + epoch / (2 * step_size)); x = np.abs(epoch / step_size - 2 * cycle + 1)
    return initial_lr + (max_lr - initial_lr) * np.maximum(0, (1 - x))

LR_SCHEDULER_REGISTRY = {'constant': constant_lr, 'linear': linear_decay_lr, 'exponential': exponential_decay_lr, 'cyclical': cyclical_lr}

def create_dynamic_model(config):
    H1 = config.get('H1', 64); H2 = config.get('H2', 64)
    activation1_type = config.get('activation1', 'prelu'); activation2_type = config.get('activation2', 'prelu')
    np.random.seed(config.get('seed', 42))
    act1 = nn.Tanh() if activation1_type == 'tanh' else nn.PReLU()
    act2 = nn.Tanh() if activation2_type == 'tanh' else nn.PReLU()
    return nn.Sequential(nn.Linear(1, H1), act1, nn.Linear(H1, H2), act2, nn.Linear(H2, 1))

# ==============================================================================
# Ø§Ù„Ù‚Ø³Ù… 2: Ø§Ù„Ø£ÙˆØ±ÙƒØ³ØªØ±Ø§ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ¯ÙˆØ§Ù„Ù‡ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
# ==============================================================================

def create_ultimate_genome():
    """
    ÙŠÙ†Ø´Ø¦ Ø¬ÙŠÙ†ÙˆÙ…Ù‹Ø§ ÙƒØ§Ù…Ù„Ø§Ù‹ ÙŠØµÙ Ø§Ù„Ø¨Ù†ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙ†Ø´ÙŠØ·ØŒ ÙˆØ§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù….
    """
    config = {
        'H1': random.choice([16, 32, 64]), 'H2': random.choice([16, 32, 64]),
        'activation1': random.choice(['prelu', 'tanh']), 'activation2': random.choice(['prelu', 'tanh']),
        'seed': random.randint(0, 10000)
    }
    lr_strategy_type = random.choice(['constant', 'linear', 'exponential', 'cyclical'])
    lr_params = {'type': lr_strategy_type, 'initial_lr': 10**random.uniform(-3, -1.5)}
    if lr_strategy_type == 'exponential':
        lr_params['decay_rate'] = random.uniform(0.99, 0.999)
    elif lr_strategy_type == 'cyclical':
        lr_params['max_lr'] = lr_params['initial_lr'] * random.uniform(2, 5)
        lr_params['step_size'] = random.choice([150, 250, 350])
    config['lr_strategy'] = lr_params
    return config

def run_ultimate_trial(config, X_train, y_train, X_test, y_test):
    """
    ØªØ¬Ø±ÙŠ ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ø­Ø¯Ø© ÙƒØ§Ù…Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø´Ø§Ù…Ù„ Ù…Ù† Ø§Ù„Ø¬ÙŠÙ†ÙˆÙ….
    """
    lr_config = config['lr_strategy']
    config_str = (f"H1={config['H1']}, Act1={config['activation1']}, H2={config['H2']}, Act2={config['activation2']}, "
                  f"LR_Strat={lr_config['type']}, Init_LR={lr_config['initial_lr']:.4f}")
    print(f"\n--- ðŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¬ÙŠÙ†ÙˆÙ…: {config_str} ---")

    model = create_dynamic_model(config)
    params = list(model.parameters())
    adam_state = [{'m': np.zeros_like(p.value), 'v': np.zeros_like(p.value)} for p in params]
    beta1, beta2, eps = 0.9, 0.999, 1e-8
    epochs = 1000
    weight_decay = 1e-4
    
    scheduler_fn = LR_SCHEDULER_REGISTRY.get(lr_config['type'], constant_lr)
    scheduler_args = {**lr_config, 'total_epochs': epochs}

    for epoch in range(1, epochs + 1):
        current_lr = scheduler_fn(epoch, **scheduler_args)
        y_pred = model(X_train)
        loss = ((y_pred + (y_train * -1)) * (y_pred + (y_train * -1))).sum()
        model.zero_grad()
        loss.backward()
        for i, p in enumerate(params):
            p.value -= weight_decay * p.value
            adam_state[i]['m'] = beta1 * adam_state[i]['m'] + (1 - beta1) * p.grad
            adam_state[i]['v'] = beta2 * adam_state[i]['v'] + (1 - beta2) * (p.grad**2)
            m_hat = adam_state[i]['m'] / (1 - beta1**epoch)
            v_hat = adam_state[i]['v'] / (1 - beta2**epoch)
            p.value -= current_lr * m_hat / (np.sqrt(v_hat) + eps)
            
    test_pred = model(X_test)
    final_loss = np.mean((test_pred.value - y_test.value)**2)
    print(f"-> Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {final_loss:.5f}")
    return final_loss

def ultimate_orchestrator(n_trials=50):
    """
    ÙŠÙ†Ø³Ù‚ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ù…Ù„Ø©.
    """
    print("ðŸŽ¼ Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ø£ÙˆØ±ÙƒØ³ØªØ±Ø§ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (v4 - ØªØ·ÙˆØ± ÙƒÙ„ Ø´ÙŠØ¡!) ðŸŽ¼")
    
    np.random.seed(42)
    X_data = np.linspace(-5, 5, 100)[:, np.newaxis]
    y_data = np.sin(X_data) + np.cos(X_data * 0.5) + np.random.randn(100, 1) * 0.2
    train_indices = np.random.choice(100, 80, replace=False)
    test_indices = np.setdiff1d(np.arange(100), train_indices)
    X_train, y_train = Node(X_data[train_indices]), Node(y_data[train_indices])
    X_test, y_test = Node(X_data[test_indices]), Node(y_data[test_indices])

    best_config = {}
    best_loss = float('inf')
    log = []

    for i in range(n_trials):
        print(f"\n--- [Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„ÙƒØ¨Ø±Ù‰ {i+1}/{n_trials}] ---")
        candidate_config = create_ultimate_genome()
        final_loss = run_ultimate_trial(candidate_config, X_train, y_train, X_test, y_test)
        log.append({'config': candidate_config, 'loss': final_loss})
        if final_loss < best_loss:
            best_loss = final_loss
            best_config = candidate_config
            print(f"ðŸ† *** Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© Ø´Ø§Ù…Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©! ***")

    print("\n\n--- ðŸ Ø§ÙƒØªÙ…Ù„ ØªØ·ÙˆØ± Ø§Ù„Ø£ÙˆØ±ÙƒØ³ØªØ±Ø§ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ðŸ ---")
    print("Ø£ÙØ¶Ù„ ØªÙƒÙˆÙŠÙ† Ø´Ø§Ù…Ù„ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡:")
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… json.dumps Ù„Ù„Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù†Ø¸Ù…Ø©
    print(json.dumps(best_config, indent=2))
    print(f"\n  Ø£Ù‚Ù„ Ø®Ø³Ø§Ø±Ø© Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ù‚Ù‚Ø©: {best_loss:.5f}")
    
    return best_config, log

# ==============================================================================
# Ø§Ù„Ù‚Ø³Ù… 3: Ø§Ù„ØªÙ†ÙÙŠØ°
# ==============================================================================
if __name__ == '__main__':
    start_time = time.time()
    best_config, full_log = ultimate_orchestrator(n_trials=50)
    end_time = time.time()
    print(f"\nØ¥Ø¬Ù…Ø§Ù„ÙŠ ÙˆÙ‚Øª Ø§Ù„Ø¨Ø­Ø«: {end_time - start_time:.2f} Ø«Ø§Ù†ÙŠØ©.")
